
# Modeling

Once the exploration analysis done, we pursue with the modelization of the profile of our arrestees and victims. As a gentle reminder, the aim of our project is to build the typical profil drawn by the database in order to know whether or not race has a strong influence and thus, to consider if there is objectively a partial legal force.

## Modeling - Arrestees

### With a normal distribution

```{r , include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
  
  
```{r warning=FALSE, message=FALSE, echo = FALSE}

Arrestation= Police_arrest_made %>%
  group_by(`Arrestee's Age`,`Arrestee's Race`, `Arrestee's Gender`, `Drugs or Alcohol Present` ) %>%
  summarize(arrestation=n())


```


We would first build the model for arrestee’s profile. In order to do so, we use a gaussian General Linear Model to shape our dependent variable, the number of arrestation, explain by our independent variables: the age, race and gender of the arrestee and if it was under the influence of drugs and alcool. We use dummy variables to better understand the impact of the outcomes.

```{r model_arrestation_normal}
model <- glm(
  arrestation ~ `Arrestee's Age`+`Arrestee's Race`+`Arrestee's Gender` + `Drugs or Alcohol Present`,
  data = Arrestation
)

```

We observe that ceteris paribus, approximatively 49 arrestations (intercept) would be done in our reference case as we could observe in the table \@ref(tab:Table-estimate). The reference case is the average of possibility between all combinations possible with our variable references and here, since we have dummy variables, it encompasses  the lowest probable profile of arrestees: black women. This reference level gives the irreductible number of arrestations the Chapel Hill Police District would operate for this category on this subset of pupulation over 9 years. 

The arrestee Age has a negative effect: it depicts that higher is the age of the arrestee lower is the probability to encounter this person, since a high age would decrease, ceteris paribus, the number of arrestations in fine by the exact value of the age of the arrestee. This coefficient is significant.

The other significant coefficients are the gender of the arrestee and the presence of drugs and alcool, the last information is divided into two significant dummy variables. Being a male would higher by almost 29 additional arrestations, ceteris paribus. The male gender has a strong effect: + 58.10% of arrestations.
Drugs and alcohol dummy variables have as wellspring effects: The unknown presence of those substances decrease our outcome by approx. 16 arrestations (-15.63), ceteris paribus, while their presence would increase by almost 19 arrestations (18.83). 

Unfortunately, this model does not give us the expect insight on race: being white is positively correlated with the number of arrestation, making white race a more probable characteristic in the typical arrestee profile. We should have intuitively deducted that indeed, however the coefficient is not significant. 


```{r Table-estimate,warning=FALSE, message=FALSE, echo = FALSE}


broom::tidy(model) %>% 
  mutate(p.value = paste(round(p.value, 4), pval_star(p.value))) %>%
  select(-statistic) %>%
  kable( 
  caption = "Information about the estimate of the model", digits = 2
  ) %>% 
  kable_styling(
  bootstrap_options = "striped"
  )

```

Those informations brought by the model are themselves accurate? To know it, we have to analyse the goodness of fit of the model and explore in more details the distribution of the errors and also the distribution or our data.

In Figure \@ref(fig:Residuals), we observe exterme value, which have a lot of weight concerning the distribution of our errors.


```{r Residuals, fig.cap="Residuals of the models about the arrested", warning=FALSE, message=FALSE, echo = FALSE}

#Bad prediction when the number of arrestation  increase

augment_model<- broom::augment(model)

p1<- augment_model %>% 
  ggplot(aes(x=arrestation,y=.resid)) + 
  geom_hline(yintercept = 0, colour = "bisque3", size= 1) + 
  geom_line() + 
  labs(
    x = "Arrestation", 
    y = "Residuals",
    title = "Bad prediction when the number\nof arrestation increase",
    subtitle = "Nomral distribution ")



p2<- Arrestation %>% 
ggplot()+
  geom_histogram(aes(arrestation))  +
  labs(
    title = "Arrestation distribution",
    x= "Number of arrestation",
    y= "Number of standard profile")

(p1 + p2) +
  plot_annotation(
    title = "Weight of the extreme value")


```

With the following table \@ref(tab:Table-Arrestation-2), we have a look on the extreme values of the above figures. 
The tricky exercise here is to understand that the extreme values are not: they are the frequency of the profiles of  people arrested: the white male with drugs and alcohol is y very common profile in the database, wherease the other combinations, representing one single person, exceed 180 different single profiles.
```{r Table-Arrestation, warning=FALSE, message=FALSE, echo = FALSE}

Arrestation %>%
  arrange( desc(arrestation)) %>%
  head(7) %>%
   kable( 
  caption = "Standard profile depending of the frequency of arrestations ", digits = 2
  ) %>% 
  kable_styling(
  bootstrap_options = "striped"
  )

```
 
 
Concerning the predictions of our response variable "Arrestation", we can observe in the figure \@ref(fig:Prediction-arrestation), that our predictions and our observed values overlap, which is good news.
On the other hand, our model has difficulty in predicting extreme values, especially for the white race.

```{r Prediction-arrestation, fig.cap="Prediction", warning=FALSE, message=FALSE, echo = FALSE}


augment_model %>% ggplot() +
  geom_boxplot(aes(x=Arrestee.s.Race, y= arrestation, col="Observed")) +
  geom_boxplot(aes(x=Arrestee.s.Race, y= .fitted, col="Fitted" )) +
  labs(title = "Well goodness-of fit",
       subtitle = "Between the observed and fitted value",
       x= "Race",
       y= "Arrestation",
       col= "Model")


```

But unfortunately we are not totally satisfied with our model: the residues are very high and our data does not follow a normal dsitribution. We can improve our model by changing the distribution. Therefore, now we do the same model and use a poisson distribution.

### With a poisson distribution 

Normal distribution describes continuous data which have a symmetric distribution, with a characteristic 'bell' shape, which is not the case in our data, as we observed in the figure \@ref(fig:Residuals). Therefore, to analyze the leeway of improvement of our model, we use the Poisson distribution and expect lower residuals and a better overall fit.

```{r model-arrestation-poisson}

model_poisson <- glm(
  arrestation ~ `Arrestee's Age`+`Arrestee's Race`+`Arrestee's Gender` + `Drugs or Alcohol Present`,
  data = Arrestation, family = "poisson"
)

```


All our estimates are significant with 0.1% except for the variable race as we observe in the table  \@ref(tab:Table-estimate-poisson). By comparison with the normal model (\@ref(tab:Table-estimate)) the variable sginificance are the same. Only the intercept is modified, which is smaller than previously, 3.84 instead of 49.24. The effect of the estimate remains similar and as previously, variable race isn't significant. 


```{r Table-estimate-poisson,warning=FALSE, message=FALSE, echo = FALSE}

broom::tidy(model_poisson) %>% 
  mutate(p.value = paste(round(p.value, 4), pval_star(p.value))) %>%
  select(-statistic) %>%
  kable( 
  caption = "Information about the estimate of the model", digits = 2
  ) %>% 
  kable_styling(
  bootstrap_options = "striped"
  )

```

In the figure, \@ref(tab:Table-estimate-poisson) we compare the distribution of the errors between the gaussian and the Poisson models. The first tables depicts the distribution of the residuals at the same scale (until -200). The gaussian residuals are greater than Poisson ones: the latter distribution better fits our dataset.

In a second time, we zoom in on the Poisson residuals to observe their distribution into an tiny interval of [-10, 10], which is pretty good.

```{r Residuals-poisson, fig.cap="Residuals of the models about the arrested", warning=FALSE, message=FALSE, echo = FALSE}

#Bad prediction when the number of arrestation  increase
augment_model_poisson<- broom::augment(model_poisson)


p11<- augment_model_poisson %>% 
  ggplot(aes(x=arrestation,y=.resid)) + 
  geom_hline(yintercept = 0, colour = "bisque3", size= 1) + 
  geom_line() + 
  labs(
    x = "Arrestation", 
    y = "Residuals",
    title = "Bad prediction when the number\nof arrestation increase",
    subtitle = "Poisson distribution") +
  scale_y_continuous(limits = c(-10,200))

p17<- augment_model_poisson %>% 
  ggplot(aes(x=arrestation,y=.resid)) + 
  geom_hline(yintercept = 0, colour = "bisque3", size= 1) + 
  geom_line() + 
  labs(
    x = "Arrestation", 
    y = "Residuals",
    title = "Zoom on the residuals distribution",
    subtitle = "Poisson distribution") 

((p11 + p1)/p17) +
  plot_annotation(
    title = "Distribution of the residuals")


```



In the next model, we add the type of arrest and whether the criminal is armed or not, in order to explain our response variable "Arrest". And this time, we use directly a poisson distribution.

```{r warning=FALSE, message=FALSE, echo = FALSE}


Arrestation_2= Police_arrest_made %>%
  group_by(`Arrestee's Age`,`Arrestee's Race`, `Arrestee's Gender`, `Drugs or Alcohol Present`, `Type of Arrest`,`Weapon Present`) %>%
  filter(`Arrestee's Race`=="Black" | `Arrestee's Race` =="White") %>%
  filter(`Drugs or Alcohol Present`!= "Unknown") %>%
  na.omit()%>%
  summarize(arrestation=n())

```

```{r model-arrestation-2, warning=FALSE, message=FALSE, echo = TRUE}

model_Arrestation_2 = glm(arrestation ~ `Arrestee's Age`+`Arrestee's Race`+`Arrestee's Gender` + `Drugs or Alcohol Present` + `Type of Arrest` + `Weapon Present`,family="poisson", data= Arrestation_2)

```


Compare to our previous model with less variables, the significance of the estimates does not change. 

The type of arrestations "Taken into custody" and being "Unarmed" are significant with 0.1%, as we observe in the table \@ref(tab:Table-Arrestation-2).


```{r Table-Estimate, warning=FALSE, message=FALSE, echo = FALSE}

broom::tidy(model_Arrestation_2) %>% 
  mutate(p.value = paste(round(p.value, 4), pval_star(p.value))) %>%
  filter(p.value<0.001) %>%
  select(-statistic) %>%
  kable( 
  caption = "Information about the estimate of the model", digits = 2
  ) %>% 
  kable_styling(
  bootstrap_options = "striped"
  )
```


The table \@ref(tab:Table-Arrestation-2) gives the most frequent profiles contained in our databases of arrested persons, including now type of arrests and drug or alcohol impaired. Again, the profils of young white males are the most frequent, and they are systematically not armed but impaired.

```{r Table-Arrestation-2, warning=FALSE, message=FALSE, echo = FALSE}

Arrestation_2 %>%
  arrange( desc(arrestation)) %>%
  head(7) %>%
   kable( 
  caption = "Information about the estimate of the model", digits = 2
  ) %>% 
  kable_styling(
  bootstrap_options = "striped"
  )

```


 


## Modelling - Victim

### With a normal distribution

The second modelization aims to model the profil of victims. Again, we use the simplest model possible: the gaussian General Linear model, to estimate the number of victims by population characteristics over 9 years. The variable taking into account the age, race and gender of the victims.
We reproduce the distinguo gaussian-Poisson as we did for arrestees' profiling in order to demonstrate again the better fit of the data by the use of a Poisson.

```{r warning=FALSE, message=FALSE, echo = FALSE}

Victim <- Incident_reports %>%
  group_by(`Victim Age`,`Victim Gender`,`Victim Race`) %>%
  summarize(Victim=n())

```
Like for the model of the arrestee profile, we improve our modelization by running after a Poisson model below:

```{r model-victim}

model_victim = glm(Victim ~ `Victim Age` + `Victim Gender` + `Victim Race` , data= Victim)

```

Again, we have a model with dummy variables (automatically created by the use of character variables, the software estimate the impact of the non-reference levels), so we have to first consider our reference level, which is black women, as we observe in the table \@ref(tab:Table-estimate-victim). The number of victims for those population, neglecting the age is approximatively 201 cases (200.56). This number is significant.

Considering now our variables: The age is negatively correlated with the number of victims by decreasing it with an effect of -2.70! We expect thus a larger number of young victims, since this coefficient is reliable since significant. Being white is a risk factor: more victims are white and this is represented by the strong positive effect of the dummy variable, significant as well, called `Victim Race'White: +86.79 so additional 87 number of victims by comparison to our reference level.

Here, we have a strong racial effect: black people report less in our data set: they seem to be less subject to misuse, aggressions, and other cases of abi. than white people. But exogeneous causes can cause it: white persons are maybe wealthier or adopt a behavioral attracting thiefs, perhaps black people are reluctant to fill a complaint in police office because of some pression, lack of interest or expecting disparagement, for exmaple.

Our database cannot help any supporting justification with those assumptions, because it does not contain any data to verify them.

Finally, the interesting point remains that gender is not significant, in spite of the negative effect (in the model, but a benefit in reality) to be a male. Being a male decreases, ceteris paribus, the number of cases by approximatively 13 victims (13.42).


```{r Table-estimate-victim,warning=FALSE, message=FALSE, echo = FALSE}
augment_model_victim<- broom::augment(model_victim)


broom::tidy(model_victim) %>% 
  mutate(p.value = paste(round(p.value, 4), pval_star(p.value))) %>%
  select(-statistic) %>%
  kable( 
  caption = "Information about the estimate of the model", digits = 2
  ) %>% 
  kable_styling(
  bootstrap_options = "striped"
  )

```
 
 The errors are distributed around zero, leading to low errors scores, satisfying! But some errors remains as outliers, Errors value follow a gaussian distribution here as well, illustrating quite a poisson curve.

 In Figure \@ref(fig:Residuals-victim), we observe 

```{r Residuals-victim, fig.cap="Residuals of the models about the model victim", warning=FALSE, message=FALSE, echo = FALSE}

#Bad prediction when the number of arrestation  increase

p3<- augment_model_victim %>% 
  ggplot(aes(x=Victim,y=.resid)) + 
  geom_hline(yintercept = 0, colour = "bisque3", size= 1) + 
  geom_line() + 
  labs(
    x = "Number of victims", 
    y = "Residuals",
    title = "Bad prediction when the number\nof arrestation increase",
    subtitle = "Normal distribution")

p4<- Victim %>% 
ggplot()+
  geom_histogram(aes(Victim))  +
  labs(
    title = "Dstribution around 0",
    subtitle = "Victim distribution",
    x= "Number of victim",
    y= "Number of standard profile")

(p3 + p4) +
  plot_annotation(
    title = "Residuals variation")



```

In order to well undersant what is inside the extreme value as we obsevre  the figure \@ref(fig:Residuals-victim), we present the seven typical profil of a victim , in the table \@ref(tab:Table-distribution-Victim), we notice that the victim is all the time a young white person.


```{r Table-distribution-Victim, fig.cap="Dstribution of the residuals", warning=FALSE, message=FALSE, echo = FALSE}

Victim %>%
  arrange( desc(Victim)) %>%
  head(7) %>%
   kable( 
  caption = "Information about the estimate of the model", digits = 2
  ) %>% 
  kable_styling(
  bootstrap_options = "striped"
  )

```


The box plot of the model’s goodness of fit points out that our model better predict black than white people: the dummy variable struggles to well fit the observation, as we notice in the figure \@ref(fig:Prediction-model-victim)

```{r Prediction-model-victim, fig.cap="Prediction victim model", warning=FALSE, message=FALSE, echo = FALSE}

augment_model_victim %>% ggplot() +
  geom_boxplot(aes(x=Victim.Race, y= Victim, col="Observed")) +
  geom_boxplot(aes(x=Victim.Race, y= .fitted, col="Fitted" )) +
  labs(title = "Well goodness-of fit",
       subtitle = "Between the observed and fitted value",
       x= "Race",
       y= "Number of victim",
       col= "Model")

```

Like the model with arrested, we made an error making the assumption that are data follow a normal distribution, but it was not the case as we note obersving the distribution of the data in the figure \@ref(fig:Residuals-model-victim). Therefore, now we assume that our data follow a poisson distribution.


```{r model-victim-poisson}

model_victim_poisson = glm(Victim ~ `Victim Age` + `Victim Gender` + `Victim Race` ,family = "poisson", data= Victim)

```

Again, we have a model with dummy variables, so we have to first consider our reference level, which is black women again, as we observe in the table \@ref(tab:Table-estimate-victim-poisson). All the variables are significant as in the previsous table with a normal distribution \@ref(tab:Table-estimate-victim). But here, all the variables are significant with 1%, which it was not the case previously. Here the varaibles male is negatively signicant, and before it was positively correlated. 

It is more correct to use this following  table \@ref(tab:Table-estimate-victim-poisson). 



```{r Table-estimate-victim-poisson,warning=FALSE, message=FALSE, echo = FALSE}
augment_model_victim_poisson<- broom::augment(model_victim_poisson)


broom::tidy(model_victim_poisson) %>% 
  mutate(p.value = paste(round(p.value, 4), pval_star(p.value))) %>%
  select(-statistic) %>%
  kable( 
  caption = "Information about the estimate of the model", digits = 2
  ) %>% 
  kable_styling(
  bootstrap_options = "striped"
  )

```

Plotting our residuals with a poisson distribution we note that we are pretty good,\@ref(fig:Residuals-victim-poisson)

```{r Residuals-victim-poisson, fig.cap="Residuals of the models about the model victim", warning=FALSE, message=FALSE, echo = FALSE}

#Bad prediction when the number of arrestation  increase

p13 <-augment_model_victim_poisson %>% 
  ggplot(aes(x=Victim,y=.resid)) + 
  geom_hline(yintercept = 0, colour = "bisque3", size= 1) + 
  geom_line() + 
  labs(
    x = "Number of victims", 
    y = "Residuals",
    title = "Bad prediction when the number\nof arrestation increase",
    subtitle = "Poisson distribution") +
  scale_y_continuous(limits = c(-200,400))

p15 <-augment_model_victim_poisson %>% 
  ggplot(aes(x=Victim,y=.resid)) + 
  geom_hline(yintercept = 0, colour = "bisque3", size= 1) + 
  geom_line() + 
  labs(
    x = "Number of victims", 
    y = "Residuals",
    title = "Zoom on the residuals distribution",
    subtitle = "Poisson distribution")

 
((p13 + p3)/ p15) +
  plot_annotation(
    title = "Residuals variation")




```


## Classifying

Finally, because no variable concerning the arrestee's race was significant, we would now see with a machine learning process if the black box is able to well predict the skin color of the person arrested.

If the process succeeds, then there would be an evidence that black or white race might have an affect on arrestation in the District of Chapel Hill because they can be predicted by the variables we selected for the model: the characteristics of the arrestations would be a key factor to forecast the skin color of the arrestee.

If not, then our data base does not allow us to demonstrate any bias in the judicial system of a county if ad only if our samples are respresentative  of our population. If this is not the case, then further investigation have be done in order to minimize the sampling bias we face here.

So, we filtered again the data to have only black and white arrestees and mutate all the character variables as factors, so as to be handled by the functions, which require factors only. We proceed to a sammpling for having a training and a test sets with which we would train and verify the analytical process.

This one is a Random Forest: Through a multiplicity of random samples we did before: our model has to predict the race pf arrestees with respect of the characteristics of the observations included in the samples. By computing an average of their results, we determine the model's ability to predict.

In the figure \@ref(fig:Prediction-model-race), we plot the cofusion matrix.


```{r Prediction-model-race, fig.cap="Race prediction model", fig.asp = 0.7, fig.width = 7, fig.align = 'center', warning=FALSE, message=FALSE, echo = FALSE}

PAM <- Police_arrest_made %>%
  clean_names() %>%
  filter(arrestees_race == c("Black","White")) %>%
  mutate(arrestees_race =if_else(arrestees_race == "Black", "Black", "White", missing = NA_character_) %>% factor()) %>%
  na.omit() %>% 
  select(arrestees_race, arrestees_age, type_of_arrest, drugs_or_alcohol_present, weapon_present) %>%
  mutate_if(is.character, as.factor)
  
set.seed(123)
index.tr <- c(sample(1:100, replace=FALSE, size=65),100+sample(1:100, replace=FALSE, size=65),200+sample(1:100, replace=FALSE, size=65))

arrest.tr <- PAM[index.tr,]
arrest.te <- PAM[-index.tr,]
# creation of the random forest model

mod.rf <- randomForest(formula = arrestees_race ~ arrestees_age + type_of_arrest + drugs_or_alcohol_present + weapon_present,
                       data = PAM)
# using the test set to see is good or not
pred.te <- predict(mod.rf, newdata=arrest.te, type="class")

confusionMatrix<-confusionMatrix(data=pred.te, reference = arrest.te$arrestees_race)


fourfoldplot(confusionMatrix$table)
```
 
We have to look at the sensitivity (ability to predict true positive, here black people) and specificity (ability to predict true negative, here whites), as well the accuracy: if our model well predicts in average the outcome, in the table  \@ref(tab:Table-Arrestation-2). Here, the accuracy is a little bit better than hazard (0.6611 against 0.5, considered here as hazard since we have only two outcomes). The sensitivity (.7560) is quite high but specificity is low (.5578). Our model predicts well black but have more difficulty to predict white, which was the issue we faced with our other two models: the white race level is non-significant.

Of course, we are now working with a biased sample. The learner's struggles are explained to a certain extent by this bias. The improvements we could have done would have been to create a subset of our dataset by randomly selecting 10% of black arrestees and 90% of white arrestees to respect the real proportion of Chapel Hill's population.

In this way, the learner would have work on less biased subset of observations and the results would have given us a better classification of arrestees per skin color.

 
```{r Table-Confision-matrix, fig.cap="Race prediction model", warning=FALSE, message=FALSE, echo = FALSE}

confusionMatrix$byClass %>%
    kable( 
  caption = "Output of the confusion matrix", digits = 2
  ) %>% 
  kable_styling(
  bootstrap_options = "striped"
  )
```




